{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745fd5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PDF Processing Pipeline Components...\n",
      "üìÑ Input Queue & Page Splitting Strategy Implementation\n"
     ]
    }
   ],
   "source": [
    "# PDF Processing Pipeline - Input Queue & Page Splitting Strategy\n",
    "import asyncio\n",
    "import threading\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "PARSER_VERSION = \"0.1.0\"\n",
    "\n",
    "print(\"Initializing PDF Processing Pipeline Components...\")\n",
    "print(\"üìÑ Input Queue & Page Splitting Strategy Implementation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2057d04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data structures defined: PDFPage, PageBatch\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class PDFPage:\n",
    "    \"\"\"Represents a single PDF page with metadata\"\"\"\n",
    "    page_number: int\n",
    "    document_id: str\n",
    "    content: Optional[bytes] = None\n",
    "    width: Optional[float] = None\n",
    "    height: Optional[float] = None\n",
    "    dpi: int = 150\n",
    "    processing_metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.processing_metadata.update({\n",
    "            'created_at': time.time(),\n",
    "            'status': 'queued',\n",
    "            'worker_id': None,\n",
    "            'batch_id': None\n",
    "        })\n",
    "\n",
    "@dataclass \n",
    "class PageBatch:\n",
    "    \"\"\"Represents a batch of 3 pages for processing\"\"\"\n",
    "    batch_id: str\n",
    "    pages: List[PDFPage]\n",
    "    worker_assignment: Optional[int] = None\n",
    "    context_pages: List[int] = field(default_factory=list)  # Adjacent page numbers for context\n",
    "    processing_priority: int = 1  # 1=high, 2=normal, 3=low\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if len(self.pages) > 3:\n",
    "            raise ValueError(\"Batch cannot contain more than 3 pages\")\n",
    "        # Set context pages (previous and next page numbers)\n",
    "        if self.pages:\n",
    "            first_page = min(p.page_number for p in self.pages)\n",
    "            last_page = max(p.page_number for p in self.pages)\n",
    "            self.context_pages = [first_page - 1, last_page + 1]\n",
    "\n",
    "print(\"‚úÖ Data structures defined: PDFPage, PageBatch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e6264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputQueue:\n",
    "    \"\"\"\n",
    "    Thread-safe PDF input queue with intelligent page loading\n",
    "    Handles multiple PDF documents simultaneously\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_queue_size: int = 1000):\n",
    "        self.max_queue_size = max_queue_size\n",
    "        self.page_queue = deque()\n",
    "        self.document_registry = {}  # document_id -> metadata\n",
    "        self.queue_lock = threading.Lock()\n",
    "        self.total_pages_loaded = 0\n",
    "        self.processing_stats = {\n",
    "            'documents_loaded': 0,\n",
    "            'pages_queued': 0,\n",
    "            'queue_size': 0\n",
    "        }\n",
    "    \n",
    "    def load_pdf_document(self, file_path: Path, document_id: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Load a PDF document and add all pages to the queue\n",
    "        Returns document_id for tracking\n",
    "        \"\"\"\n",
    "        if document_id is None:\n",
    "            document_id = f\"doc_{int(time.time())}_{file_path.stem}\"\n",
    "        \n",
    "        # Simulate PDF loading (in real implementation, use PyMuPDF/pdfplumber)\n",
    "        simulated_page_count = 45  # Example: 45-page financial report\n",
    "        \n",
    "        with self.queue_lock:\n",
    "            # Register document\n",
    "            self.document_registry[document_id] = {\n",
    "                'file_path': str(file_path),\n",
    "                'total_pages': simulated_page_count,\n",
    "                'loaded_at': time.time(),\n",
    "                'status': 'loading'\n",
    "            }\n",
    "            \n",
    "            # Create and queue pages\n",
    "            for page_num in range(1, simulated_page_count + 1):\n",
    "                if len(self.page_queue) >= self.max_queue_size:\n",
    "                    print(f\"‚ö†Ô∏è Queue full! Skipping page {page_num}\")\n",
    "                    break\n",
    "                    \n",
    "                page = PDFPage(\n",
    "                    page_number=page_num,\n",
    "                    document_id=document_id,\n",
    "                    width=8.5 * 72,  # Letter size in points\n",
    "                    height=11 * 72\n",
    "                )\n",
    "                \n",
    "                self.page_queue.append(page)\n",
    "                self.total_pages_loaded += 1\n",
    "            \n",
    "            # Update document status\n",
    "            self.document_registry[document_id]['status'] = 'queued'\n",
    "            self.processing_stats['documents_loaded'] += 1\n",
    "            self.processing_stats['pages_queued'] = len(self.page_queue)\n",
    "            self.processing_stats['queue_size'] = len(self.page_queue)\n",
    "        \n",
    "        print(f\"üìÑ Loaded document '{document_id}': {simulated_page_count} pages\")\n",
    "        return document_id\n",
    "    \n",
    "    def get_queue_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current queue statistics\"\"\"\n",
    "        with self.queue_lock:\n",
    "            return {\n",
    "                **self.processing_stats,\n",
    "                'documents_in_registry': len(self.document_registry),\n",
    "                'total_pages_loaded': self.total_pages_loaded\n",
    "            }\n",
    "    \n",
    "    def peek_next_pages(self, count: int = 3) -> List[PDFPage]:\n",
    "        \"\"\"Peek at next pages without removing them from queue\"\"\"\n",
    "        with self.queue_lock:\n",
    "            return list(self.page_queue)[:count]\n",
    "\n",
    "print(\"‚úÖ InputQueue class implemented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd3edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageSplitter:\n",
    "    \"\"\"\n",
    "    Intelligent page splitter with 3-page batching strategy\n",
    "    Optimized for GPU memory usage while maintaining document context\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size: int = 3, num_workers: int = 4):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_counter = 0\n",
    "        self.worker_assignments = [[] for _ in range(num_workers)]  # Track batches per worker\n",
    "        self.batch_registry = {}  # batch_id -> PageBatch\n",
    "        \n",
    "    def create_batches_from_queue(self, input_queue: InputQueue) -> List[PageBatch]:\n",
    "        \"\"\"\n",
    "        Create 3-page batches from the input queue\n",
    "        Maintains context preservation and optimal GPU memory usage\n",
    "        \"\"\"\n",
    "        batches = []\n",
    "        \n",
    "        with input_queue.queue_lock:\n",
    "            pages_available = list(input_queue.page_queue)\n",
    "            \n",
    "            # Process pages in groups of 3\n",
    "            for i in range(0, len(pages_available), self.batch_size):\n",
    "                batch_pages = pages_available[i:i + self.batch_size]\n",
    "                \n",
    "                if not batch_pages:\n",
    "                    break\n",
    "                \n",
    "                # Create batch with unique ID\n",
    "                batch_id = f\"batch_{self.batch_counter:04d}\"\n",
    "                batch = PageBatch(\n",
    "                    batch_id=batch_id,\n",
    "                    pages=batch_pages,\n",
    "                    processing_priority=self._calculate_priority(batch_pages)\n",
    "                )\n",
    "                \n",
    "                # Update page metadata\n",
    "                for page in batch_pages:\n",
    "                    page.processing_metadata['batch_id'] = batch_id\n",
    "                    page.processing_metadata['status'] = 'batched'\n",
    "                \n",
    "                batches.append(batch)\n",
    "                self.batch_registry[batch_id] = batch\n",
    "                self.batch_counter += 1\n",
    "                \n",
    "                # Remove processed pages from queue\n",
    "                for _ in range(len(batch_pages)):\n",
    "                    if input_queue.page_queue:\n",
    "                        input_queue.page_queue.popleft()\n",
    "        \n",
    "        print(f\"üì¶ Created {len(batches)} batches of {self.batch_size} pages each\")\n",
    "        return batches\n",
    "    \n",
    "    def _calculate_priority(self, pages: List[PDFPage]) -> int:\n",
    "        \"\"\"\n",
    "        Calculate processing priority based on page characteristics\n",
    "        1=high priority (cover page, financial statements)\n",
    "        2=normal priority (regular content)\n",
    "        3=low priority (appendices, footnotes)\n",
    "        \"\"\"\n",
    "        # Simple heuristic: first few pages and specific ranges get higher priority\n",
    "        page_numbers = [p.page_number for p in pages]\n",
    "        min_page = min(page_numbers)\n",
    "        \n",
    "        if min_page <= 5:  # Cover page and early content\n",
    "            return 1\n",
    "        elif 20 <= min_page <= 40:  # Typical financial statement pages\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    \n",
    "    def get_batch_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about batch creation\"\"\"\n",
    "        total_batches = len(self.batch_registry)\n",
    "        total_pages = sum(len(batch.pages) for batch in self.batch_registry.values())\n",
    "        \n",
    "        priority_distribution = {}\n",
    "        for batch in self.batch_registry.values():\n",
    "            priority = batch.processing_priority\n",
    "            priority_distribution[priority] = priority_distribution.get(priority, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            'total_batches_created': total_batches,\n",
    "            'total_pages_batched': total_pages,\n",
    "            'average_batch_size': total_pages / total_batches if total_batches > 0 else 0,\n",
    "            'priority_distribution': priority_distribution,\n",
    "            'batch_size_setting': self.batch_size\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ PageSplitter class implemented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73207bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkerDistributor:\n",
    "    \"\"\"\n",
    "    Round-robin distribution system for 4 GPU workers\n",
    "    Ensures balanced workload and optimal resource utilization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_workers: int = 4):\n",
    "        self.num_workers = num_workers\n",
    "        self.worker_queues = [deque() for _ in range(num_workers)]\n",
    "        self.worker_stats = [{'batches_assigned': 0, 'pages_assigned': 0} for _ in range(num_workers)]\n",
    "        self.current_worker = 0  # Round-robin counter\n",
    "        \n",
    "    def assign_batches_to_workers(self, batches: List[PageBatch]) -> Dict[int, List[PageBatch]]:\n",
    "        \"\"\"\n",
    "        Distribute batches across workers using round-robin strategy\n",
    "        Returns: {worker_id: [batches]}\n",
    "        \"\"\"\n",
    "        worker_assignments = {i: [] for i in range(self.num_workers)}\n",
    "        \n",
    "        for batch in batches:\n",
    "            # Assign to current worker\n",
    "            worker_id = self.current_worker\n",
    "            batch.worker_assignment = worker_id\n",
    "            \n",
    "            # Update page metadata\n",
    "            for page in batch.pages:\n",
    "                page.processing_metadata['worker_id'] = worker_id\n",
    "                page.processing_metadata['status'] = 'assigned'\n",
    "            \n",
    "            # Add to worker queue and assignment\n",
    "            self.worker_queues[worker_id].append(batch)\n",
    "            worker_assignments[worker_id].append(batch)\n",
    "            \n",
    "            # Update statistics\n",
    "            self.worker_stats[worker_id]['batches_assigned'] += 1\n",
    "            self.worker_stats[worker_id]['pages_assigned'] += len(batch.pages)\n",
    "            \n",
    "            # Move to next worker (round-robin)\n",
    "            self.current_worker = (self.current_worker + 1) % self.num_workers\n",
    "        \n",
    "        self._print_distribution_summary(worker_assignments)\n",
    "        return worker_assignments\n",
    "    \n",
    "    def _print_distribution_summary(self, assignments: Dict[int, List[PageBatch]]):\n",
    "        \"\"\"Print a summary of the worker distribution\"\"\"\n",
    "        print(\"\\\\nüîÑ Worker Distribution Summary:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for worker_id in range(self.num_workers):\n",
    "            batches = assignments[worker_id]\n",
    "            if batches:\n",
    "                page_ranges = []\n",
    "                for batch in batches:\n",
    "                    pages = [p.page_number for p in batch.pages]\n",
    "                    page_ranges.append(f\"{min(pages)}-{max(pages)}\")\n",
    "                \n",
    "                print(f\"Worker {worker_id}: {len(batches)} batches | Pages: {', '.join(page_ranges[:3])}\")\n",
    "                if len(page_ranges) > 3:\n",
    "                    print(f\"             ... and {len(page_ranges) - 3} more batches\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    def get_worker_load_balance(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze load balance across workers\"\"\"\n",
    "        total_batches = sum(stats['batches_assigned'] for stats in self.worker_stats)\n",
    "        total_pages = sum(stats['pages_assigned'] for stats in self.worker_stats)\n",
    "        \n",
    "        if total_batches == 0:\n",
    "            return {'balanced': True, 'variance': 0, 'worker_loads': []}\n",
    "        \n",
    "        avg_batches_per_worker = total_batches / self.num_workers\n",
    "        variance = sum((stats['batches_assigned'] - avg_batches_per_worker) ** 2 \n",
    "                      for stats in self.worker_stats) / self.num_workers\n",
    "        \n",
    "        worker_loads = []\n",
    "        for i, stats in enumerate(self.worker_stats):\n",
    "            worker_loads.append({\n",
    "                'worker_id': i,\n",
    "                'batches': stats['batches_assigned'],\n",
    "                'pages': stats['pages_assigned'],\n",
    "                'load_percentage': (stats['batches_assigned'] / total_batches * 100) if total_batches > 0 else 0\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'balanced': variance < 1.0,  # Considered balanced if variance < 1\n",
    "            'variance': variance,\n",
    "            'total_batches': total_batches,\n",
    "            'total_pages': total_pages,\n",
    "            'average_batches_per_worker': avg_batches_per_worker,\n",
    "            'worker_loads': worker_loads\n",
    "        }\n",
    "    \n",
    "    def simulate_worker_processing(self, worker_id: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Simulate processing for a specific worker\n",
    "        Returns list of batch IDs that would be processed\n",
    "        \"\"\"\n",
    "        worker_queue = self.worker_queues[worker_id]\n",
    "        processing_order = [batch.batch_id for batch in worker_queue]\n",
    "        \n",
    "        print(f\"\\\\nüñ•Ô∏è Worker {worker_id} Processing Simulation:\")\n",
    "        print(f\"   Queue length: {len(processing_order)} batches\")\n",
    "        print(f\"   Processing order: {processing_order[:5]}\")  # Show first 5\n",
    "        if len(processing_order) > 5:\n",
    "            print(f\"   ... and {len(processing_order) - 5} more\")\n",
    "        \n",
    "        return processing_order\n",
    "\n",
    "print(\"‚úÖ WorkerDistributor class implemented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfd737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextPreserver:\n",
    "    \"\"\"\n",
    "    Maintains document context and relationships between adjacent pages\n",
    "    Ensures cross-page elements (spanning tables, sections) are handled properly\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.document_structure = {}  # document_id -> structure info\n",
    "        self.page_relationships = {}  # page_id -> adjacent pages info\n",
    "        self.cross_page_elements = {}  # Elements that span multiple pages\n",
    "        \n",
    "    def analyze_document_structure(self, document_id: str, pages: List[PDFPage]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze the overall structure of a document for context preservation\n",
    "        \"\"\"\n",
    "        page_count = len(pages)\n",
    "        page_numbers = [p.page_number for p in pages]\n",
    "        \n",
    "        structure = {\n",
    "            'document_id': document_id,\n",
    "            'total_pages': page_count,\n",
    "            'page_range': (min(page_numbers), max(page_numbers)),\n",
    "            'sections': self._identify_sections(pages),\n",
    "            'potential_spanning_elements': self._detect_spanning_elements(pages)\n",
    "        }\n",
    "        \n",
    "        self.document_structure[document_id] = structure\n",
    "        return structure\n",
    "    \n",
    "    def _identify_sections(self, pages: List[PDFPage]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Identify document sections based on page patterns\n",
    "        (In real implementation, this would use layout analysis)\n",
    "        \"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        # Simulate section detection for financial documents\n",
    "        page_numbers = [p.page_number for p in pages]\n",
    "        total_pages = len(pages)\n",
    "        \n",
    "        if total_pages > 0:\n",
    "            # Common financial document sections\n",
    "            sections.extend([\n",
    "                {'name': 'Cover & TOC', 'pages': [1, 2, 3], 'type': 'front_matter'},\n",
    "                {'name': 'Management Discussion', 'pages': list(range(4, 12)), 'type': 'narrative'},\n",
    "                {'name': 'Financial Statements', 'pages': list(range(12, 25)), 'type': 'financial_data'},\n",
    "                {'name': 'Notes to Statements', 'pages': list(range(25, 40)), 'type': 'detailed_notes'},\n",
    "                {'name': 'Appendices', 'pages': list(range(40, max(page_numbers) + 1)), 'type': 'supplementary'}\n",
    "            ])\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def _detect_spanning_elements(self, pages: List[PDFPage]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Detect elements that might span across pages\n",
    "        \"\"\"\n",
    "        spanning_elements = []\n",
    "        \n",
    "        # Simulate detection of cross-page elements\n",
    "        for i, page in enumerate(pages[:-1]):  # Don't check last page\n",
    "            next_page = pages[i + 1]\n",
    "            \n",
    "            # Check for potential spanning tables (common in financial reports)\n",
    "            if self._might_have_spanning_table(page, next_page):\n",
    "                spanning_elements.append({\n",
    "                    'type': 'table',\n",
    "                    'start_page': page.page_number,\n",
    "                    'end_page': next_page.page_number,\n",
    "                    'confidence': 0.8,\n",
    "                    'description': f'Potential table spanning pages {page.page_number}-{next_page.page_number}'\n",
    "                })\n",
    "        \n",
    "        return spanning_elements\n",
    "    \n",
    "    def _might_have_spanning_table(self, page1: PDFPage, page2: PDFPage) -> bool:\n",
    "        \"\"\"\n",
    "        Simple heuristic to detect potential spanning tables\n",
    "        (In real implementation, would analyze actual content)\n",
    "        \"\"\"\n",
    "        # Financial statements often have tables spanning pages 15-20\n",
    "        return 15 <= page1.page_number <= 20 and page2.page_number == page1.page_number + 1\n",
    "    \n",
    "    def get_context_for_batch(self, batch: PageBatch) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get contextual information for a batch to improve processing\n",
    "        \"\"\"\n",
    "        if not batch.pages:\n",
    "            return {}\n",
    "        \n",
    "        document_id = batch.pages[0].document_id\n",
    "        page_numbers = [p.page_number for p in batch.pages]\n",
    "        \n",
    "        context = {\n",
    "            'batch_id': batch.batch_id,\n",
    "            'document_id': document_id,\n",
    "            'page_numbers': page_numbers,\n",
    "            'adjacent_pages': {\n",
    "                'before': min(page_numbers) - 1 if min(page_numbers) > 1 else None,\n",
    "                'after': max(page_numbers) + 1\n",
    "            },\n",
    "            'section_info': self._get_section_for_pages(document_id, page_numbers),\n",
    "            'spanning_elements': self._get_spanning_elements_for_pages(document_id, page_numbers),\n",
    "            'processing_hints': self._generate_processing_hints(page_numbers)\n",
    "        }\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def _get_section_for_pages(self, document_id: str, page_numbers: List[int]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get section information for specific pages\"\"\"\n",
    "        if document_id not in self.document_structure:\n",
    "            return []\n",
    "        \n",
    "        sections = self.document_structure[document_id]['sections']\n",
    "        relevant_sections = []\n",
    "        \n",
    "        for section in sections:\n",
    "            if any(page_num in section['pages'] for page_num in page_numbers):\n",
    "                relevant_sections.append({\n",
    "                    'name': section['name'],\n",
    "                    'type': section['type'],\n",
    "                    'overlap_pages': [p for p in page_numbers if p in section['pages']]\n",
    "                })\n",
    "        \n",
    "        return relevant_sections\n",
    "    \n",
    "    def _get_spanning_elements_for_pages(self, document_id: str, page_numbers: List[int]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get spanning elements that affect these pages\"\"\"\n",
    "        if document_id not in self.document_structure:\n",
    "            return []\n",
    "        \n",
    "        spanning_elements = self.document_structure[document_id]['potential_spanning_elements']\n",
    "        relevant_elements = []\n",
    "        \n",
    "        for element in spanning_elements:\n",
    "            if (element['start_page'] in page_numbers or \n",
    "                element['end_page'] in page_numbers or\n",
    "                (element['start_page'] < min(page_numbers) and element['end_page'] > max(page_numbers))):\n",
    "                relevant_elements.append(element)\n",
    "        \n",
    "        return relevant_elements\n",
    "    \n",
    "    def _generate_processing_hints(self, page_numbers: List[int]) -> List[str]:\n",
    "        \"\"\"Generate hints for optimal processing based on page characteristics\"\"\"\n",
    "        hints = []\n",
    "        \n",
    "        min_page = min(page_numbers)\n",
    "        max_page = max(page_numbers)\n",
    "        \n",
    "        if min_page <= 5:\n",
    "            hints.append(\"Early pages: Expect cover page, TOC, executive summary\")\n",
    "        \n",
    "        if 10 <= min_page <= 25:\n",
    "            hints.append(\"Financial statements likely: Use enhanced table detection\")\n",
    "        \n",
    "        if 25 <= min_page <= 40:\n",
    "            hints.append(\"Notes section: Complex formatting, detailed tables\")\n",
    "        \n",
    "        if max_page >= 40:\n",
    "            hints.append(\"Appendices: May contain charts, supplementary data\")\n",
    "        \n",
    "        return hints\n",
    "\n",
    "print(\"‚úÖ ContextPreserver class implemented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage and Demonstration\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üöÄ PIPELINE DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize the pipeline components\n",
    "input_queue = InputQueue(max_queue_size=1000)\n",
    "page_splitter = PageSplitter(batch_size=3, num_workers=4)\n",
    "worker_distributor = WorkerDistributor(num_workers=4)\n",
    "context_preserver = ContextPreserver()\n",
    "\n",
    "# Simulate loading a financial document\n",
    "sample_pdf_path = Path(\"data/raw/msft_10k_2024.pdf\")  # Example path\n",
    "document_id = input_queue.load_pdf_document(sample_pdf_path, \"MSFT_10K_2024\")\n",
    "\n",
    "# Check queue status\n",
    "queue_status = input_queue.get_queue_status()\n",
    "print(f\"\\\\nüìä Queue Status: {queue_status}\")\n",
    "\n",
    "# Get some pages for context analysis\n",
    "sample_pages = input_queue.peek_next_pages(45)  # Get all pages\n",
    "if sample_pages:\n",
    "    # Analyze document structure\n",
    "    structure = context_preserver.analyze_document_structure(document_id, sample_pages)\n",
    "    print(f\"\\\\nüìã Document Structure Analysis:\")\n",
    "    print(f\"   Total Pages: {structure['total_pages']}\")\n",
    "    print(f\"   Sections: {len(structure['sections'])}\")\n",
    "    print(f\"   Spanning Elements: {len(structure['potential_spanning_elements'])}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Pipeline components initialized and demonstrated!\")\n",
    "print(\"Ready for batch processing and worker distribution...\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batches and distribute to workers\n",
    "print(\"\\\\nüîÑ BATCH CREATION AND WORKER DISTRIBUTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create 3-page batches from the queue\n",
    "batches = page_splitter.create_batches_from_queue(input_queue)\n",
    "\n",
    "# Get batch statistics\n",
    "batch_stats = page_splitter.get_batch_statistics()\n",
    "print(f\"\\\\nüì¶ Batch Statistics:\")\n",
    "for key, value in batch_stats.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Distribute batches to workers using round-robin\n",
    "worker_assignments = worker_distributor.assign_batches_to_workers(batches)\n",
    "\n",
    "# Analyze load balance\n",
    "load_balance = worker_distributor.get_worker_load_balance()\n",
    "print(f\"\\\\n‚öñÔ∏è Load Balance Analysis:\")\n",
    "print(f\"   Balanced: {load_balance['balanced']}\")\n",
    "print(f\"   Variance: {load_balance['variance']:.2f}\")\n",
    "print(f\"   Average batches per worker: {load_balance['average_batches_per_worker']:.1f}\")\n",
    "\n",
    "print(\"\\\\nüë• Worker Load Distribution:\")\n",
    "for worker_load in load_balance['worker_loads']:\n",
    "    print(f\"   Worker {worker_load['worker_id']}: \"\n",
    "          f\"{worker_load['batches']} batches ({worker_load['load_percentage']:.1f}%) \"\n",
    "          f\"| {worker_load['pages']} pages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f425875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate context preservation for specific batches\n",
    "print(\"\\\\nüß† CONTEXT PRESERVATION DEMONSTRATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show context for first few batches\n",
    "for i, batch in enumerate(batches[:3]):  # Show first 3 batches\n",
    "    context = context_preserver.get_context_for_batch(batch)\n",
    "    \n",
    "    print(f\"\\\\nüìÑ Batch {batch.batch_id} Context:\")\n",
    "    print(f\"   Pages: {context['page_numbers']}\")\n",
    "    print(f\"   Adjacent pages: Before={context['adjacent_pages']['before']}, \"\n",
    "          f\"After={context['adjacent_pages']['after']}\")\n",
    "    \n",
    "    if context['section_info']:\n",
    "        print(f\"   Sections: {[s['name'] for s in context['section_info']]}\")\n",
    "    \n",
    "    if context['spanning_elements']:\n",
    "        print(f\"   Spanning elements: {len(context['spanning_elements'])} detected\")\n",
    "    \n",
    "    if context['processing_hints']:\n",
    "        print(f\"   Processing hints: {context['processing_hints'][0]}\")\n",
    "\n",
    "# Simulate processing for one worker\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "worker_distributor.simulate_worker_processing(0)  # Worker 0\n",
    "worker_distributor.simulate_worker_processing(1)  # Worker 1\n",
    "\n",
    "print(\"\\\\n‚úÖ Context preservation and worker simulation complete!\")\n",
    "print(\"\\\\nüí° Key Benefits Demonstrated:\")\n",
    "print(\"   ‚Ä¢ 3-page batches optimize GPU memory usage\")\n",
    "print(\"   ‚Ä¢ Round-robin ensures balanced workload\")\n",
    "print(\"   ‚Ä¢ Context preservation maintains document relationships\")\n",
    "print(\"   ‚Ä¢ Cross-page elements are properly tracked\")\n",
    "print(\"   ‚Ä¢ Processing hints optimize extraction strategies\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
